return(all_publications)
}
get_all_coauthors = function(my_id, me=NULL) {
all_publications = get_all_publications(my_id)
if (is.null(me))
me = strsplit(get_profile(my_id)$name, " ")[[1]][2]
# make the author list a character vector
all_authors = sapply(all_publications$author, as.character)
# split it over ", "
all_authors = unlist(sapply(all_authors, strsplit, ", "))
names(all_authors) = NULL
# remove "..." and yourself
all_authors = all_authors[!(all_authors %in% c("..."))]
all_authors = all_authors[-grep(me, all_authors)]
# make a data frame with authors by decreasing number of appearance
all_authors = data.frame(name=factor(all_authors,
levels=names(sort(table(main_authors),decreasing=TRUE))))
}
get_abstract = function(pub_id, author_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
# get summary of all publications of an author (from their google scholar id)
library(scholar)
my_id = "cQm68jUAAAAJ"
all_publications = get_all_publications(my_id)
dim(all_publications)
table(all_publications$year)
summary(all_publications$cites)
# find all co-authors
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
all_authors = get_all_coauthors(my_id, me="Campa")
# find all co-authors
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
library(scholar)
# Define the id for Richard Feynman
id <- 'B7vSqZsAAAAJ'
# Get his profile and print his name
l <- get_profile(id)
l$name
# Get his profile and print his name
l <- get_profile(id)
# Get his citation history, i.e. citations to his work in a given year
get_citation_history(id)
# Get his publications (a large data frame)
get_publications(id)
# Get his profile and print his name
l <- get_profile(id)
l
# Get his profile and print his name
l <- get_profile(id)
l$name
library(scholar)
library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)
#Define the person to analyze
scholar_id="bruHK0YAAAAJ"
#either load existing file of publications
#or get a new one from Google Scholar
#delete the file to force an update
if (file.exists('citations.Rds'))
{
cites <- readRDS('citations.Rds')
} else {
#get citations
cites <- scholar::get_citation_history(scholar_id)
saveRDS(cites,'citations.Rds')
}
period_1_start = 2009
period_2_start = 2015
cites_1 <- cites %>% dplyr::filter((year>=period_1_start & year<period_2_start ))
#remove last year since it's not a full year
cites_2 <- cites %>% dplyr::filter((year>=period_2_start & year<2021 ))
fit1=lm(cites ~ year, data = cites_1)
fit2=lm(cites ~ year, data = cites_2)
inc1 = fit1$coefficients["year"]
inc2 = fit2$coefficients["year"]
print(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))
# combine data above into single data frame
#add a variable to indicate period 1 and period 2
cites_1$group = "1"
cites_2$group = "2"
cites_df = rbind(cites_1,cites_2)
xlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]
#make the plot and show linear fit lines
p1 <- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) +
geom_point(size = I(4)) +
geom_smooth(method="lm",aes(group = group), se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
#open a new graphics window
#note that this is Windows specific. Use quartz() for MacOS
ww=5; wh=5;
windows(width=ww, height=wh)
print(p1)
dev.print(device=png,width=ww,height=wh,units="in",res=600,file="citations.png")
#get all pubs for an author (or multiple)
if (file.exists('publications.Rds'))
{
publications <- readRDS('publications.Rds')
} else {
#get citations
publications <- scholar::get_publications(scholar_id)
saveRDS(publications,'publications.Rds')
}
glimpse(publications)
#here I only want publications since 2015
pub_reduced <- publications %>% dplyr::filter(year>2014)
ifdata <- scholar::get_impactfactor(pub_reduced$journal)
#Google SCholar collects all kinds of 'publications'
#including items other than standard peer-reviewed papers
#this sorts and removes some non-journal entries
iftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()
knitr::kable(iftable)
allauthors = list()
if (file.exists('allauthors.Rds'))
{
allauthors <- readRDS('allauthors.Rds')
} else {
for (n in 1:nrow(publications))
{
allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)
}
saveRDS(allauthors,'allauthors.Rds')
}
library(dplyr)
library(knitr)
mu::mu.library(bibliometrix)
mu::mu.library("bibliometrix")
library(bibliometrix)
#Define the person to analyze
scholar_id="cQm68jUAAAAJ"
#get citations
cites <- scholar::get_citation_history(scholar_id)
saveRDS(cites,'citations.Rds')
fit1=lm(cites ~ year, data = cites)
inc1 = fit1$coefficients["year"]
print(sprintf('Annual increase is %f',inc1))
xlabel
cites$year[seq(1,nrow(cites),by=2)]
xlabel = cites$year[seq(1,nrow(cites),by=2)]
#make the plot and show linear fit lines
p1 <- ggplot(data = cites, aes(year, cites, colour=group, shape=group)) +
geom_point(size = I(4)) +
geom_smooth(method="lm",aes(group = group), se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
#open a new graphics window
#note that this is Windows specific. Use quartz() for MacOS
ww=5; wh=5;
windows(width=ww, height=wh)
print(p1)
#make the plot and show linear fit lines
p1 <- ggplot(data = cites, aes(year, cites)) +
geom_point(size = I(4)) +
geom_smooth(method="lm", se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
print(p1)
dev.print(device=png,width=ww,height=wh,units="in",res=600,file="my citations.png")
#get citations
publications <- scholar::get_publications(scholar_id)
saveRDS(publications,'publications.Rds')
glimpse(publications)
#here I only want publications since 2015
# pub_reduced <- publications %>% dplyr::filter(year>2014)
ifdata <- scholar::get_impactfactor(publications$journal)
ifdata
#Google SCholar collects all kinds of 'publications'
#including items other than standard peer-reviewed papers
#this sorts and removes some non-journal entries
iftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()
knitr::kable(iftable)
allauthors = list()
for (n in 1:nrow(publications))
{
allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)
}
saveRDS(allauthors,'allauthors.Rds')
allauthors
?htmlTreeParse
# get summary of all publications of an author (from their google scholar id)
library(scholar)
my_id = "cQm68jUAAAAJ"
all_publications = get_all_publications(my_id)
# get summary of all publications of an author (from their google scholar id)
library(scholar)
get_all_publications = function(authorid) {
# initializing the publication list
all_publications = NULL
# initializing a counter for the citations
cstart = 0
# initializing a boolean that check if the loop should continue
notstop = TRUE
while (notstop) {
new_publications = try(get_publications(my_id, cstart=cstart), silent=TRUE)
if (class(new_publications)=="try-error") {
notstop = FALSE
} else {
# append publication list
all_publications = rbind(all_publications, new_publications)
cstart=cstart+20
}
}
return(all_publications)
}
get_all_coauthors = function(my_id, me=NULL) {
all_publications = get_all_publications(my_id)
if (is.null(me))
me = strsplit(get_profile(my_id)$name, " ")[[1]][2]
# make the author list a character vector
all_authors = sapply(all_publications$author, as.character)
# split it over ", "
all_authors = unlist(sapply(all_authors, strsplit, ", "))
names(all_authors) = NULL
# remove "..." and yourself
all_authors = all_authors[!(all_authors %in% c("..."))]
all_authors = all_authors[-grep(me, all_authors)]
# make a data frame with authors by decreasing number of appearance
all_authors = data.frame(name=factor(all_authors,
levels=names(sort(table(main_authors),decreasing=TRUE))))
}
get_abstract = function(pub_id, author_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
mu::mu.library("pubmed.mineR")
library(pubmed.mineR)
example_abstracts <- "./Data/abstract-sensorimot-set.txt"
myabs=readabs(example_abstracts)
myabs
gtest <- Genewise(myabs)
?Genewise
?searchabsL
?contextSearch
contextSearch(myabs, "implicit")
cstest <- contextSearch(myabs, "implicit")
cstest
install.packages("easyPubMed")
library(easyPubMed)
my_query <- 'sensorimotor adaptation'
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
dim(my_abstracts_txt)
my_abstracts_xml <- fetch_pubmed_data(pubmed_id_list = my_entrez_id)
my_PM_list <- articles_to_list(pubmed_data = my_abstracts_xml)
print(substr(my_PM_list[4], 1, 510))
install.packages("RefManageR")
libraryRefManageR)
library(RefManageR)
if (interactive() && !httr::http_error("https://eutils.ncbi.nlm.nih.gov/")){
file.name <- system.file("Bib", "RJC.bib", package="RefManageR")
bib <- ReadBib(file.name)
bib <- LookupPubMedID(bib[[101:102]])
toBiblatex(GetPubMedRelated(bib, batch.mode = TRUE, max.results = 2,
return.sim.scores = TRUE, return.related.ids = TRUE))
GetPubMedRelated(bib, batch.mode = FALSE, max.results = c(2, 2))
}
install.packages("bibtext")
library(bibtext)
library(RefManageR)
library(bibtext)
install.packages("bibtext")
library(easyPubMed)
##
try({
## Display some contents
data("EPMsamples")
#display Query String used for collecting the data
print(EPMsamples$NUBL_1618$qry_st)
#Get records
BL_list <- EPMsamples$NUBL_1618$rec_lst
cat(BL_list[[1]])
# cast PM recort to data.frame
BL_df <- article_to_df(BL_list[[1]], max_chars = 0)
print(BL_df)
}, silent = TRUE)
## Not run:
## Query PubMed, retrieve a selected citation and format it as a data frame
dami_query <- "Damiano Fantini[AU] AND 2017[PDAT]"
dami_on_pubmed <- get_pubmed_ids(dami_query)
dami_abstracts_xml <- fetch_pubmed_data(dami_on_pubmed)
dami_abstracts_list <- articles_to_list(dami_abstracts_xml)
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
article_to_df(pubmedArticle = dami_abstracts_list[[2]], autofill = TRUE, max_chars = 300)[1:2,]
install.packages("pubmedR")
library(pubmedR)
query <- "'sensorimotor adaptation'*[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2000:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
api_key <- "af507337d86f1d0a0bdbdf1a22e80a417608"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2000:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
D$query_translation
query <- "sensorimotor[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2000:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
D$query_translation
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2000:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
D$query_translation
D
res$query_translation
res$query_translation
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 1950:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 1950:2020[DP]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA] AND Journal Article[PT]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation[Title/Abstract] AND Journal Article[PT]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation[Title/Abstract] AND english[LA]"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
query <- "sensorimotor adaptation"
res <- pmQueryTotalCount(query = query, api_key = api_key)
res$total_count
res$query_translation
D <- pmApiRequest(query = query, limit = res$total_count, api_key = NULL)
library(bibliometrix)
M <- pmApi2df(D)
str(M)
?convert2df
M <- convert2df(D, dbsource = "pubmed", format = "api")
results <- biblioAnalysis(M)
summary(results)
library(easyPubMed)
my_query <- 'sensorimotor adaptation'
library(easyPubMed)
my_query <- 'sensorimotor adaptation'
# query
my_entrez_id <- get_pubmed_ids(my_query)
# extract abstracts from query
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
my_abstracts_xml <- fetch_pubmed_data(pubmed_id_list = my_entrez_id)
head(my_abstracts_xml)
length(my_abstracts_xml)
length(my_abstracts_txt)
my_abstracts_txt[1]
my_PM_list <- articles_to_list(pubmed_data = my_abstracts_xml)
my_PM_list <- articles_to_list(pubmed_data = my_abstracts_xml)
print(substr(my_PM_list[4], 1, 510))
##
try({
## Display some contents
data("EPMsamples")
#display Query String used for collecting the data
print(EPMsamples$NUBL_1618$qry_st)
#Get records
BL_list <- EPMsamples$NUBL_1618$rec_lst
cat(BL_list[[1]])
# cast PM recort to data.frame
BL_df <- article_to_df(BL_list[[1]], max_chars = 0)
print(BL_df)
}, silent = TRUE)
names(my_entrez_id)
library(easyPubMed)
library(easyPubMed)
my_query <- 'sensorimotor adaptation'
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
my_abstracts_xml <- fetch_pubmed_data(pubmed_id_list = my_entrez_id)
my_PM_list <- articles_to_list(pubmed_data = my_abstracts_xml)
print(substr(my_PM_list[4], 1, 510))
names(my_entrez_id)
names(my_abstracts_txt)
names(my_abstracts_xml)
names(my_PM_list)
names(my_query)
names(EPMsamples)
##
try({
## Display some contents
data("EPMsamples")
#display Query String used for collecting the data
print(EPMsamples$NUBL_1618$qry_st)
#Get records
BL_list <- EPMsamples$NUBL_1618$rec_lst
cat(BL_list[[1]])
# cast PM recort to data.frame
BL_df <- article_to_df(BL_list[[1]], max_chars = 0)
print(BL_df)
}, silent = TRUE)
#display Query String used for collecting the data
print(EPMsamples$NUBL_1618$qry_st)
cat(BL_list[[1]])
print(BL_df)
## Not run:
## Query PubMed, retrieve a selected citation and format it as a data frame
dami_query <- "Damiano Fantini[AU] AND 2017[PDAT]"
dami_on_pubmed <- get_pubmed_ids(dami_query)
dami_on_pubmed
dami_abstracts_xml <- fetch_pubmed_data(dami_on_pubmed)
dami_abstracts_xml
dami_abstracts_list <- articles_to_list(dami_abstracts_xml)
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
article_to_df(pubmedArticle = dami_abstracts_list[[2]], autofill = TRUE, max_chars = 300)[1:2,]
my_query <- 'sensorimotor adaptation'
# query
dami_on_pubmed <- get_pubmed_ids(dami_query)
# extract abstracts from query
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
dami_abstracts_xml <- fetch_pubmed_data(dami_on_pubmed)
dami_abstracts_list <- articles_to_list(dami_abstracts_xml)
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
# query
dami_on_pubmed <- get_pubmed_ids(my_query)
# extract abstracts from query
dami_abstracts_txt <- fetch_pubmed_data(dami_on_pubmed, format = "abstract")
dami_abstracts_xml <- fetch_pubmed_data(dami_on_pubmed)
dami_abstracts_list <- articles_to_list(dami_abstracts_xml)
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
length(dami_abstracts_list)
dami_abstracts_list$keywords[1:10]
dami_abstracts_list[[1]]$keywords[1:10]
dami_abstracts_list[[1]]
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)$keywords[1:10]
article_to_df(pubmedArticle = dami_abstracts_list[[1:2]], autofill = FALSE)
str(article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE))
papers_list <- article_to_df(pubmedArticle = dami_abstracts_list[[1]], autofill = FALSE)
library(plyr)
library(mu)
names(papers_list)
keywordsData <- ldply(papers_list, .(pmid), summarise,
N = lengthunique(keywords))
library(mu)
library(plyr)
keywordsData <- ldply(papers_list, .(pmid), summarise,
N = lengthunique(keywords))
keywordsData <- ldply(papers_list, .(pmid), summarise,
N = 1)
keywordsData <- ldply(papers_list, .(pmid), summarize,
N = 1)
keywordsData <- ddply(papers_list, .(pmid), summarise,
N = 1)
keywordsData <- ddply(papers_list, .(pmid), summarise,
N = lengthunique(keywords))
head(keywordsData)
keywordsData
keywordsData <- ddply(papers_list, .(pmid), summarise,
N = lengthunique(keywords))
head(keywordsData)
keywordsData <- llply(papers_list, .(pmid), summarise,
N = lengthunique(keywords))
str(papers_list)
article_to_df(pubmedArticle = dami_abstracts_list[[100]], autofill = FALSE)
unique(papers_list$pmid)
keywordsData <- NULL
keywordsData <- NULL
for(i in 1:length(papers_list))
{
keywordsData[[i]] <- unique(papers_list[[i]]$keywords)
}
i
keywordsData <- list()
for(i in 1:length(papers_list))
{
keywordsData[[i]] <- unique(papers_list[[i]]$keywords)
}
keywordsData[[i]]
unique(papers_list[[i]]$keywords)
papers_list[[i]]
unique(article_to_df(pubmedArticle = papers_list[[i]], autofill = FALSE)$keywords)
article_to_df(pubmedArticle = papers_list[[i]], autofill = FALSE)
unique(article_to_df(pubmedArticle = dami_abstracts_list[[i]], autofill = FALSE)$keywords)
keywordsData <- list()
for(i in 1:length(papers_list))
{
keywordsData[[i]] <- unique(article_to_df(pubmedArticle = dami_abstracts_list[[i]], autofill = FALSE)$keywords)
}
keywordsData[[100]]
length(dami_abstracts_list)
keywordsData <- list()
for(i in 1:length(dami_abstracts_list))
{
keywordsData[[i]] <- unique(article_to_df(pubmedArticle = dami_abstracts_list[[i]], autofill = FALSE)$keywords)
}
keywordsData[[100]]
