# make a data frame with authors by decreasing number of appearance
all_authors = data.frame(name=factor(all_authors,
levels=names(sort(table(main_authors),decreasing=TRUE))))
}
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1))]
all_authors
all_authors$name
names(which(table(all_authors$name)>1))
all_authors$name %in% names(which(table(all_authors$name)>1))
all_authors
all_authors[all_authors$name %in% names(which(table(all_authors$name)>1))]
all_authors[,all_authors$name %in% names(which(table(all_authors$name)>1))]
all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
library(ggplot2)
p = ggplot(main_authors, aes(x=name) + geom_bar(fill=brewer.pal(3, "Set2")[2]) +
)
p = ggplot(main_authors, aes(x=name)) + geom_bar(fill=brewer.pal(3, "Set2")[2]) +
xlab("co-author") + theme_bw() + theme(axis.text.x = element_text(angle=90, hjust=1))
main_authors
all_authors
main_authors
all_authors$name %in% names(which(table(all_authors$name)>1))
all_authors = get_all_coauthors(my_id, me="Campa")
all_authors
get_abstract = function(pub_id, my_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
my_id, "&citation_for_view=", my_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
get_abstract = function(pub_id, my_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
my_id, "&citation_for_view=", my_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = sapply(all_publications$pubid, get_abstract)
# abstract word analysis
get_all_abstracts = function(my_id) {
all_publications = get_all_publications(my_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library(XML)
mu::mu.library("XML")
mu::mu.library(tm)
mu::mu.library("tm")
library("XML")
all_abstracts = get_all_abstracts(my_id)
library("tm")
all_abstracts = get_all_abstracts(my_id = my_id)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
all_abstracts = get_all_abstracts(my_id = my_id)
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, my_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
my_id, "&citation_for_view=", my_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
return(all_abstracts)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, my_id) {
print(pub_id)
authord_id <- my_id
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
authord_id, "&citation_for_view=", authord_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = get_all_abstracts(my_id)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, author_id, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, authord_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
authord_id, "&citation_for_view=", authord_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = get_all_abstracts(my_id)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(list(all_publications$pubid, author_id), get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, author_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, author_id) {
print(pub_id)
id <- author_id
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
id, "&citation_for_view=", id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = get_all_abstracts(my_id)
# abstract word analysis
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_abstract = function(pub_id, author_id) {
print(pub_id)
id <<- author_id
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
id, "&citation_for_view=", id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
all_abstracts = get_all_abstracts(my_id)
all_abstracts
all_publications
get_abstract(all_publications)
get_abstract(all_publications, my_id)
all_publications
get_abstract(all_publications[1,], my_id)
get_abstract(all_publications[1,], my_id)
all_publications[1,]
pub_id <- all_publications[1,]
pub_id <- all_publications[1,]$pubid
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
author_id <- my_id
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_url
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
devtools::install_github("amcrisan/Adjutant")
library(adjutant)
runAdjutant()
devtools::install_github("amcrisan/Adjutant")
library(adjutant)
runAdjutant()
runAdjutant()
library(adjutant)
runAdjutant()
devtools::install_github("amcrisan/Adjutant")
library(adjutant)
runAdjutant()
library(adjutant)
library(dplyr)
library(ggplot2)
library(tidytext) #for stop words
#also set a seed - there is some randomness in the analysis.
set.seed(416)
df<-processSearch("(outbreak OR epidemic OR pandemic) AND genom*",retmax=2000)
##
tidy_df<-tidyCorpus(corpus = df)
df
##
df<-processSearch("(outbreak OR epidemic OR pandemic) AND genom*",retmax=200)
runAdjutant()
library(scholar)
# Define the id for Richard Feynman
id <- 'B7vSqZsAAAAJ'
# Get his profile and print his name
l <- get_profile(id)
l$name
l
# Get his citation history, i.e. citations to his work in a given year
get_citation_history(id)
# Get his publications (a large data frame)
get_publications(id)
library(scholar)
install.packages("scholar")
install.packages("scholar")
library(scholar)
# Define the id for Richard Feynman
id <- 'B7vSqZsAAAAJ'
my_id = "cQm68jUAAAAJ"
all_publications = get_all_publications(my_id)
get_all_publications = function(authorid) {
# initializing the publication list
all_publications = NULL
# initializing a counter for the citations
cstart = 0
# initializing a boolean that check if the loop should continue
notstop = TRUE
while (notstop) {
new_publications = try(get_publications(my_id, cstart=cstart), silent=TRUE)
if (class(new_publications)=="try-error") {
notstop = FALSE
} else {
# append publication list
all_publications = rbind(all_publications, new_publications)
cstart=cstart+20
}
}
return(all_publications)
}
get_all_coauthors = function(my_id, me=NULL) {
all_publications = get_all_publications(my_id)
if (is.null(me))
me = strsplit(get_profile(my_id)$name, " ")[[1]][2]
# make the author list a character vector
all_authors = sapply(all_publications$author, as.character)
# split it over ", "
all_authors = unlist(sapply(all_authors, strsplit, ", "))
names(all_authors) = NULL
# remove "..." and yourself
all_authors = all_authors[!(all_authors %in% c("..."))]
all_authors = all_authors[-grep(me, all_authors)]
# make a data frame with authors by decreasing number of appearance
all_authors = data.frame(name=factor(all_authors,
levels=names(sort(table(main_authors),decreasing=TRUE))))
}
get_abstract = function(pub_id, author_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
# get summary of all publications of an author (from their google scholar id)
library(scholar)
my_id = "cQm68jUAAAAJ"
all_publications = get_all_publications(my_id)
dim(all_publications)
table(all_publications$year)
summary(all_publications$cites)
# find all co-authors
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
all_authors = get_all_coauthors(my_id, me="Campa")
# find all co-authors
main_authors = all_authors[all_authors$name %in% names(which(table(all_authors$name)>1)),]
library("XML")
all_abstracts = get_all_abstracts(my_id)
get_all_abstracts = function(author_id) {
all_publications = get_all_publications(author_id)
all_abstracts = sapply(all_publications$pubid, get_abstract)
return(all_abstracts)
}
library("XML")
all_abstracts = get_all_abstracts(my_id)
library(scholar)
# Define the id for Richard Feynman
id <- 'B7vSqZsAAAAJ'
# Get his profile and print his name
l <- get_profile(id)
l$name
# Get his profile and print his name
l <- get_profile(id)
# Get his citation history, i.e. citations to his work in a given year
get_citation_history(id)
# Get his publications (a large data frame)
get_publications(id)
# Get his profile and print his name
l <- get_profile(id)
l
# Get his profile and print his name
l <- get_profile(id)
l$name
library(scholar)
library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)
#Define the person to analyze
scholar_id="bruHK0YAAAAJ"
#either load existing file of publications
#or get a new one from Google Scholar
#delete the file to force an update
if (file.exists('citations.Rds'))
{
cites <- readRDS('citations.Rds')
} else {
#get citations
cites <- scholar::get_citation_history(scholar_id)
saveRDS(cites,'citations.Rds')
}
period_1_start = 2009
period_2_start = 2015
cites_1 <- cites %>% dplyr::filter((year>=period_1_start & year<period_2_start ))
#remove last year since it's not a full year
cites_2 <- cites %>% dplyr::filter((year>=period_2_start & year<2021 ))
fit1=lm(cites ~ year, data = cites_1)
fit2=lm(cites ~ year, data = cites_2)
inc1 = fit1$coefficients["year"]
inc2 = fit2$coefficients["year"]
print(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))
# combine data above into single data frame
#add a variable to indicate period 1 and period 2
cites_1$group = "1"
cites_2$group = "2"
cites_df = rbind(cites_1,cites_2)
xlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]
#make the plot and show linear fit lines
p1 <- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) +
geom_point(size = I(4)) +
geom_smooth(method="lm",aes(group = group), se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
#open a new graphics window
#note that this is Windows specific. Use quartz() for MacOS
ww=5; wh=5;
windows(width=ww, height=wh)
print(p1)
dev.print(device=png,width=ww,height=wh,units="in",res=600,file="citations.png")
#get all pubs for an author (or multiple)
if (file.exists('publications.Rds'))
{
publications <- readRDS('publications.Rds')
} else {
#get citations
publications <- scholar::get_publications(scholar_id)
saveRDS(publications,'publications.Rds')
}
glimpse(publications)
#here I only want publications since 2015
pub_reduced <- publications %>% dplyr::filter(year>2014)
ifdata <- scholar::get_impactfactor(pub_reduced$journal)
#Google SCholar collects all kinds of 'publications'
#including items other than standard peer-reviewed papers
#this sorts and removes some non-journal entries
iftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()
knitr::kable(iftable)
allauthors = list()
if (file.exists('allauthors.Rds'))
{
allauthors <- readRDS('allauthors.Rds')
} else {
for (n in 1:nrow(publications))
{
allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)
}
saveRDS(allauthors,'allauthors.Rds')
}
library(dplyr)
library(knitr)
mu::mu.library(bibliometrix)
mu::mu.library("bibliometrix")
library(bibliometrix)
#Define the person to analyze
scholar_id="cQm68jUAAAAJ"
#get citations
cites <- scholar::get_citation_history(scholar_id)
saveRDS(cites,'citations.Rds')
fit1=lm(cites ~ year, data = cites)
inc1 = fit1$coefficients["year"]
print(sprintf('Annual increase is %f',inc1))
xlabel
cites$year[seq(1,nrow(cites),by=2)]
xlabel = cites$year[seq(1,nrow(cites),by=2)]
#make the plot and show linear fit lines
p1 <- ggplot(data = cites, aes(year, cites, colour=group, shape=group)) +
geom_point(size = I(4)) +
geom_smooth(method="lm",aes(group = group), se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
#open a new graphics window
#note that this is Windows specific. Use quartz() for MacOS
ww=5; wh=5;
windows(width=ww, height=wh)
print(p1)
#make the plot and show linear fit lines
p1 <- ggplot(data = cites, aes(year, cites)) +
geom_point(size = I(4)) +
geom_smooth(method="lm", se = F, size=1.5) +
scale_x_continuous(name = "Year", breaks = xlabel, labels = xlabel) +     scale_y_continuous("Citations according to Google Scholar") +
theme_bw(base_size=14) + theme(legend.position="none") +
geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 22%",color="black",size=5.5) +
geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 43%",color="black",size=5.5)
print(p1)
dev.print(device=png,width=ww,height=wh,units="in",res=600,file="my citations.png")
#get citations
publications <- scholar::get_publications(scholar_id)
saveRDS(publications,'publications.Rds')
glimpse(publications)
#here I only want publications since 2015
# pub_reduced <- publications %>% dplyr::filter(year>2014)
ifdata <- scholar::get_impactfactor(publications$journal)
ifdata
#Google SCholar collects all kinds of 'publications'
#including items other than standard peer-reviewed papers
#this sorts and removes some non-journal entries
iftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()
knitr::kable(iftable)
allauthors = list()
for (n in 1:nrow(publications))
{
allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)
}
saveRDS(allauthors,'allauthors.Rds')
allauthors
?htmlTreeParse
# get summary of all publications of an author (from their google scholar id)
library(scholar)
my_id = "cQm68jUAAAAJ"
all_publications = get_all_publications(my_id)
# get summary of all publications of an author (from their google scholar id)
library(scholar)
get_all_publications = function(authorid) {
# initializing the publication list
all_publications = NULL
# initializing a counter for the citations
cstart = 0
# initializing a boolean that check if the loop should continue
notstop = TRUE
while (notstop) {
new_publications = try(get_publications(my_id, cstart=cstart), silent=TRUE)
if (class(new_publications)=="try-error") {
notstop = FALSE
} else {
# append publication list
all_publications = rbind(all_publications, new_publications)
cstart=cstart+20
}
}
return(all_publications)
}
get_all_coauthors = function(my_id, me=NULL) {
all_publications = get_all_publications(my_id)
if (is.null(me))
me = strsplit(get_profile(my_id)$name, " ")[[1]][2]
# make the author list a character vector
all_authors = sapply(all_publications$author, as.character)
# split it over ", "
all_authors = unlist(sapply(all_authors, strsplit, ", "))
names(all_authors) = NULL
# remove "..." and yourself
all_authors = all_authors[!(all_authors %in% c("..."))]
all_authors = all_authors[-grep(me, all_authors)]
# make a data frame with authors by decreasing number of appearance
all_authors = data.frame(name=factor(all_authors,
levels=names(sort(table(main_authors),decreasing=TRUE))))
}
get_abstract = function(pub_id, author_id) {
print(pub_id)
paper_url = paste0("http://scholar.google.com/citations?view_op=view_citation&hl=fr&user=",
author_id, "&citation_for_view=", author_id,":", pub_id)
paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
return(paper_abstract)
}
mu::mu.library("pubmed.mineR")
library(pubmed.mineR)
example_abstracts <- "./Data/abstract-sensorimot-set.txt"
myabs=readabs(example_abstracts)
myabs
gtest <- Genewise(myabs)
?Genewise
?searchabsL
?contextSearch
contextSearch(myabs, "implicit")
cstest <- contextSearch(myabs, "implicit")
cstest
install.packages("easyPubMed")
library(easyPubMed)
my_query <- 'sensorimotor adaptation'
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
dim(my_abstracts_txt)
my_abstracts_xml <- fetch_pubmed_data(pubmed_id_list = my_entrez_id)
my_PM_list <- articles_to_list(pubmed_data = my_abstracts_xml)
print(substr(my_PM_list[4], 1, 510))
